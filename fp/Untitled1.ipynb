{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import re\n",
    "import json\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from zhon.hanzi import punctuation\n",
    "jieba.enable_paddle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_congress = []\n",
    "data1 = xlrd.open_workbook(\"government_report1.xlsx\")\n",
    "table1 = data1.sheet_by_index(1)\n",
    "nrows = table1.nrows\n",
    "ncols = table1.ncols\n",
    "for row in range(1, nrows):\n",
    "    excel_rows = []\n",
    "    for col in range(ncols):\n",
    "        cell_value = table1.cell(row,col).value\n",
    "        excel_rows.append(cell_value)\n",
    "    people_congress.append(excel_rows)\n",
    "people_congress = pd.DataFrame(people_congress)\n",
    "len(people_congress)\n",
    "#会议文本位于people_congress[i][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_congress = []\n",
    "data2 = xlrd.open_workbook(\"party_congress.xlsx\")\n",
    "table2 = data2.sheet_by_index(0)\n",
    "nrows = table2.nrows\n",
    "ncols = table2.ncols\n",
    "for row in range(1, nrows):\n",
    "    excel_rows = []\n",
    "    for col in range(ncols):\n",
    "        cell_value = table2.cell(row,col).value\n",
    "        excel_rows.append(cell_value)\n",
    "    party_congress.append(excel_rows)\n",
    "len(party_congress)\n",
    "\n",
    "people_congress = pd.DataFrame(people_congress)\n",
    "people_congress.to_pickle(\"./people_congress.pkl\")#存储没有清洗的元数据\n",
    "#会议文本位于party_congress[i][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_str = punctuation #导入标点符号库\n",
    "stopwords = [line.strip() for line in open('stopwords.txt', 'r', encoding = 'utf-8').readlines()]\n",
    "#导入停顿词库\n",
    "jieba.load_userdict(\"dictionary.txt\") #导入自定义词典（未使用，可按照词典筛选词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#党代清洗\n",
    "party_text = [] #去除符号、标点等候保留的纯文本\n",
    "for year in range(len(party_congress)):\n",
    "    a = party_congress[year][7]\n",
    "    a = a.replace('\\n','').replace('\\r','').replace(' ','') #去除换行，空格\n",
    "    for i in punctuation:\n",
    "        a = a.replace(i,'') #去除标点符号\n",
    "    party_text.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建观感良好的data frame\n",
    "year = []\n",
    "for i in range(len(party_congress)):\n",
    "    t = str(party_congress[i][0])\n",
    "    r = t.rstrip('0').strip('.')\n",
    "    year.append(r)\n",
    "\n",
    "number = []\n",
    "for i in range(len(party_congress)):\n",
    "    t = str(party_congress[i][1])\n",
    "    r = t.rstrip('0').strip('.')\n",
    "    number.append(r) \n",
    "\n",
    "sub_number = []\n",
    "for i in range(len(party_congress)):\n",
    "    t = str(party_congress[i][3])\n",
    "    r = t.rstrip('0').strip('.')\n",
    "    sub_number.append(r)  \n",
    "\n",
    "doctype = []\n",
    "for i in range(len(party_congress)):\n",
    "    t = str(party_congress[i][5])\n",
    "    r = t.rstrip('0').strip('.')\n",
    "    doctype.append(r)  \n",
    "\n",
    "party_list = list(zip(year, number, sub_number, doctype, party_text))\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "data_df=pd.DataFrame(party_list, columns=['year','number','sub_number','doctypr','report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将不同年限的报告整合在一起\n",
    "def ab(data_df):\n",
    "    return','.join(data_df.values)\n",
    "data_df2 = data_df.groupby('year')['report'].apply(ab)\n",
    "data_df2 = data_df2.reset_index()\n",
    "data_df2 = data_df2.drop(0)\n",
    "data_df2 = data_df2.reset_index() \n",
    "\n",
    "data_df2.to_pickle(\"./data_df2.pkl\")#存储清洗后的数据\n",
    "#删掉第一行后出了错配问题，resetindex后解决，虽然出现了多余一列，但在后续匹配data_final时没有出现问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词函数\n",
    "def jiebait(text):\n",
    "    seglist = jieba.cut(text, cut_all = False)\n",
    "    year_word = []\n",
    "    for word in seglist:\n",
    "        if word !='\\t':\n",
    "            if word not in stopwords:\n",
    "                year_word = ['  '.join(word)]\n",
    "    filter_seglist = [fil for fil in year_word if len(fil) >= 2]\n",
    "    return filter_seglist \n",
    "#似乎不太好用，尝试用CountVectorize的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-55074c5547d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mall_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'report'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_df2' is not defined"
     ]
    }
   ],
   "source": [
    "#创建适合CountVectorize的目标数据格式\n",
    "report = []\n",
    "for i in range(len(data_df2)):\n",
    "    all_list = ['  '.join(jieba.cut(data_df2['report'][i],cut_all = False))]\n",
    "    report.append(all_list)\n",
    "data_final = dict(zip(data_df['year'],report))\n",
    "data_final_df = pd.DataFrame.from_dict(data_final).transpose()\n",
    "data_final_df.columns = ['report']\n",
    "\n",
    "data_final_df.to_pickle(\"./data_final.pkl\") #存储分词后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-837369acdcd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcount_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_final_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_dtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_final_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#将分词后数据转化为向量\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(stop_words=stopwords,min_df=2)\n",
    "x_count = count_vec.fit_transform(data_final_df.report)\n",
    "data_dtm = pd.DataFrame(x_count.toarray(),columns=count_vec.get_feature_names())\n",
    "data_dtm.index = data_final_df.index\n",
    "data_dtm\n",
    "\n",
    "pickle.dump(count_vec, open(\"./cv_stop.pkl\", \"wb\")) #存储cv数据本身作为语料库\n",
    "data_dtm.to_pickle(\"./dtm.pkl\") #documenttermmatrix数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成提次数最多得100个词及其词频\n",
    "import pickle\n",
    "data = pd.read_pickle('./dtm.pkl')\n",
    "data = data.transpose()\n",
    "data.head()\n",
    "\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(100)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict #这个记得保存一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#并将前30个词打印\n",
    "for year, top_words in top_dict.items():\n",
    "    print(year)\n",
    "    print(', '.join([word for word, count in top_words[0:29]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial这部分教程是希望将top30words加入stopword，但是comedian数据集和政府工作报告数据集存在区别，我保留了这段code，但没有执行，有必要可以执行\n",
    "from collections import Counter\n",
    "# Let's first pull out the top 30 words for each comedian\n",
    "words = []\n",
    "for year in data.columns:\n",
    "    top = [word for (word, count) in top_dict[year]]\n",
    "    for t in top:\n",
    "        words.append(t) #将频率最高得词进行保存得方法，但注意结果不是按年份，修改代码如下:\n",
    "\n",
    "words = []\n",
    "for year in data.columns:\n",
    "    top = [word for (word, count) in top_dict[year]]\n",
    "    year = []\n",
    "    for t in top:\n",
    "        year.append(t)\n",
    "    word.append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "!pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(stopwords=stopwords, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Allows charts to appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_pickle('./data_df2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "a = list(range(1977,2021))\n",
    "year_num = []\n",
    "for i in range(len(a)):\n",
    "    year.append(str(a[i]))\n",
    "year_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each year\n",
    "for index, year in enumerate(data.columns):\n",
    "    wc.generate(data_clean.report[index])\n",
    "    plt.subplot(11, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(year_num[index])\n",
    "    \n",
    "plt.show() #这里的wordcloud画不出来，先看下topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dtm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\abel\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from gensim) (1.15.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: requests in c:\\users\\abel\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.19.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\abel\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = data.transpose()\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "cv = pickle.load(open(\"./cv_stop.pkl\", \"rb\"))#导入基本语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, passes=100)\n",
    "lda.print_topics() #这里给出了针对所有文档资料的topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "import numpy\n",
    "from gensim.matutils import hellinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据时间变化的topic\n",
    "sparse_counts2 = scipy.sparse.csr_matrix(data)\n",
    "corpus2 = matutils.Sparse2Corpus(sparse_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py:293: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  convergence = np.fabs((bound - old_bound) / old_bound)\n"
     ]
    }
   ],
   "source": [
    "c = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,]\n",
    "time_slice = c\n",
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=id2word, time_slice=time_slice, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
